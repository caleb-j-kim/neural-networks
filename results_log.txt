Final Results Summary:
   Activation  Learning Rate  Epochs  Hidden Layers  Train Loss  Validation Loss  R-squared
0     sigmoid           0.01     100              2    0.362658         0.375815   0.447530
1     sigmoid           0.01     100              3    0.422838         0.384331  -0.145650
2     sigmoid           0.01     200              2    0.426309         0.389644  -0.014593
3     sigmoid           0.01     200              3    0.488600         0.424524  -0.188524
4     sigmoid           0.10     100              2    0.709743         0.691244  -0.312459
5     sigmoid           0.10     100              3    0.700959         0.654247  -0.097712
6     sigmoid           0.10     200              2    0.696021         0.711885  -0.192081
7     sigmoid           0.10     200              3    0.736089         0.917952  -0.275728
8        tanh           0.01     100              2    0.391652         0.369718   0.143868
9        tanh           0.01     100              3    0.416481         0.383880   0.049804
10       tanh           0.01     200              2    0.315921         0.359550   0.467742
11       tanh           0.01     200              3    0.398181         0.405413   0.048882
12       tanh           0.10     100              2    0.502726         0.438647   0.187436
13       tanh           0.10     100              3    0.729063         0.588467  -0.150050
14       tanh           0.10     200              2    0.482822         0.427699  -0.653229
15       tanh           0.10     200              3    0.562895         0.732206  -0.170939
16       relu           0.01     100              2    0.420418         0.409784  -1.171963
17       relu           0.01     100              3    0.398770         0.390498  -0.482006
18       relu           0.01     200              2    0.429192         0.430654  -1.466611
19       relu           0.01     200              3    0.412469         0.375411  -1.144232
20       relu           0.10     100              2    0.401253         0.435536  -0.931110
21       relu           0.10     100              3    0.456721         0.392012  -2.470158
22       relu           0.10     200              2    0.425891         0.488642  -0.446653
23       relu           0.10     200              3    0.461139         0.492866  -1.480128
